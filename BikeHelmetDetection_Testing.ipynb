{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvb_aij-hSma"
      },
      "source": [
        "# Bike Helmet Detection - Testing Notebook\n",
        "_This Notebook contains all my dev tests and the methods I implemented and used in my project_demo.py_\n",
        "\n",
        "<img src='predict\\tracked_image5.png' alt='tracked_image5.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xL-KBiOh9Nq"
      },
      "source": [
        "# Module imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rvrx0Gjmh9rX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "import threading\n",
        "from statistics import mean\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eJTLDT0iGcF"
      },
      "source": [
        "# Project paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFw3bO7jiB2w"
      },
      "outputs": [],
      "source": [
        "project_dir = r\"C:\\Development\\BikeHelmetDetection\"\n",
        "if not os.path.isdir(project_dir):\n",
        "    raise FileNotFoundError(\"Project directory not found.\")\n",
        "\n",
        "test_img_dir = os.path.join(project_dir, \"test\", \"images\")\n",
        "test_videos_dir = os.path.join(project_dir, \"test\", \"videos\")\n",
        "\n",
        "models_dir = os.path.join(project_dir, \"models\")\n",
        "medias_dir = os.path.join(project_dir, \"medias\")\n",
        "predict_dir = os.path.join(project_dir, \"predict\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qn6DL7VkMI1"
      },
      "source": [
        "# Load YOLO models\n",
        "\n",
        "We want to test and compare the predictions from three models on images, webcam and video files:\n",
        "- YOLOv8n pre-trained on Coco\n",
        "- YOLOv8n pre-trained on OpenImaveV7\n",
        "- My trained model fine-tuned on the Helmet Dataset. See Training notebooks on [Kaggle](https://www.kaggle.com/code/alexandredj/bikehelmetdetection-yolov8n-training) or [Colab](https://drive.google.com/file/d/1KGJ68orNqPCK3llccBD6_8MmcEXA1As3/view?usp=sharing).\n",
        "\n",
        "We need to find a way to detect cyclists and decide if they are wearing a helmet or not.\n",
        "Load YOLO models and list the classes we are interested in.  \n",
        "\n",
        "Models will be downloaded if not found in the project directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3UB773picZI",
        "outputId": "03277686-4b77-47cc-cdc0-448b9320b914"
      },
      "outputs": [],
      "source": [
        "# yolov8n pre-trained on COCO\n",
        "coco_yolo = YOLO('yolov8n.pt')\n",
        "print(\"COCO yolo loaded successfully\")\n",
        "print(f\"{len(coco_yolo.names)} classes\")\n",
        "for i in [0, 1]:  # person, bycicle\n",
        "    print(f\"- {i}: {coco_yolo.names[i]}\")\n",
        "print()\n",
        "\n",
        "# yolov8n pre-trained on OpenImageV7\n",
        "oiv7_yolo = YOLO('yolov8n-oiv7.pt')\n",
        "print(\"OpenImageV7 YOLO loaded successfully\", )\n",
        "print(f\"{len(oiv7_yolo.names)} classes\")\n",
        "for i in [42, 43, 322, 381, 594]:  # bicycle, bicycle helmet, man, woman, person\n",
        "    print(f\"- {i}: {oiv7_yolo.names[i]}\")\n",
        "print()\n",
        "\n",
        "# load my model, pretrained on COCO and fine-tuned on helmet dataset\n",
        "my_yolo = YOLO(os.path.join(models_dir, 'best_260424_0028.pt'))\n",
        "print(\"Fine-Tuned YOLO loaded successfully\")\n",
        "print(f\"{len(my_yolo.names)} classes\")\n",
        "for i in [0, 1]:  # without helmet, with helmet\n",
        "    print(f\"- {i}: {my_yolo.names[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UgpnX3_mVv8"
      },
      "source": [
        "# Test YOLO model on an image\n",
        "\n",
        "Draw a random image from the test folder and predict classes on it using the 3 models.  \n",
        "Test images are all from https://pxhere.com/  (CC license)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PxjEk-KamX3O",
        "outputId": "7cc35fa4-cf31-464a-92fe-4a2b65524764"
      },
      "outputs": [],
      "source": [
        "def get_test_image(img_dir=None):\n",
        "    '''\n",
        "    Get a random image from a directory, test/images by default\n",
        "\n",
        "    Args:\n",
        "        img_dir: str: path to the directory containing images\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Load a random image\n",
        "    if img_dir is None:\n",
        "        img_dir = test_img_dir\n",
        "\n",
        "    # pick a random image from the val set\n",
        "    img_file = os.path.join(img_dir, random.choice(os.listdir(img_dir)))\n",
        "    print(img_file)\n",
        "\n",
        "    return img_file\n",
        "\n",
        "def predict_on_image(img_file, model=None, classes=None):\n",
        "    '''\n",
        "    Predict and display persons and bikes in an image using the YOLO model\n",
        "\n",
        "    Args:\n",
        "        img_file: str: path to the image file\n",
        "        model: YOLO: YOLO model\n",
        "        classes: list: filter classes to predict\n",
        "    '''\n",
        "\n",
        "    # Load model\n",
        "    if model is None:\n",
        "        model = YOLO('yolov8n.pt')  # YOLOv8n model\n",
        "\n",
        "    # Set the confidence threshold\n",
        "    conf_thresh = 0.25\n",
        "\n",
        "    # Predict all classes if filter classes are not provided\n",
        "    if not classes:\n",
        "        classes = list(model.names.keys())\n",
        "\n",
        "    # predict on image\n",
        "    results = model.predict(source=img_file, classes=classes, save=True, conf=conf_thresh)\n",
        "\n",
        "    for result in results:\n",
        "\n",
        "        # Save the image\n",
        "        img_pred = os.path.join(project_dir, 'predict', os.path.split(img_file)[-1])\n",
        "        result.save(img_pred)\n",
        "\n",
        "        # Display the image\n",
        "        image = Image.open(img_pred)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')  # Turn off axis\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "img_file = get_test_image()\n",
        "\n",
        "print(f\"\\n{100*'-'}\\n# COCO YOLO #\\n{100*'-'}\")\n",
        "coco_classes = [0, 1]  # person, bicycle\n",
        "predict_on_image(img_file, model=coco_yolo, classes=coco_classes)\n",
        "\n",
        "print(f\"\\n{100*'-'}\\n# OPEN IMAGE V7 YOLO #\\n{100*'-'}\")\n",
        "oiv7_classes = [322, 594, 381, 42, 43]  # man, woman, person, bicycle, bicycle helmet\n",
        "predict_on_image(img_file, model=oiv7_yolo, classes=oiv7_classes)\n",
        "\n",
        "print(f\"\\n{100*'-'}\\n# FINE-TUNED YOLO / HELMET DATASET #\\n{100*'-'}\")\n",
        "predict_on_image(img_file, model=my_yolo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper method to parse and draw results\n",
        "\n",
        "We define here a few methods to simplify parsing the results from the models and drawing the bounding boxes on images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_results(results, model, classes=None):\n",
        "    '''\n",
        "    Parse the tracking or prediction results of a YOLO model and store them in a list of dicts\n",
        "    Each dict contains the class, confidence and bounding box coordinates\n",
        "\n",
        "    Args:\n",
        "        results: YOLO results object\n",
        "        model: YOLO: YOLO model used to predict the results\n",
        "        classes: list: list of classes to filter the results (None by default)\n",
        "    \n",
        "    Return:\n",
        "        results_list: list: list of dictionaries containing the class, confidence and bounding box coordinates\n",
        "    '''\n",
        "\n",
        "    if classes is None:\n",
        "        classes = model.names\n",
        "        \n",
        "    results_list = []\n",
        "\n",
        "    for result in results:\n",
        "        boxes = result.boxes\n",
        "        for box in boxes:\n",
        "            pred = {}\n",
        "            box_cls = int(box.cls[0])\n",
        "            if box_cls not in classes:\n",
        "                continue\n",
        "            pred['class'] = model.names[box_cls]\n",
        "            pred['confidence'] = round(float(box.conf[0]), 3)\n",
        "            pred['bbox'] = tuple(map(int, box.xyxy[0]))\n",
        "            results_list.append(pred)\n",
        "\n",
        "    return results_list\n",
        "\n",
        "def print_predictions(results_list):\n",
        "    '''\n",
        "    Print the tracking/prediction results from a YOLO model\n",
        "\n",
        "    Args:\n",
        "        results_list: list: list of dictionaries returned by parse_results\n",
        "    '''\n",
        "\n",
        "    for i, result in enumerate(results_list):\n",
        "        print(f\"Prediction {i+1}:\")\n",
        "\n",
        "        for k, v in result.items():\n",
        "            print(f'{k}: {v}')\n",
        "\n",
        "        print()\n",
        "\n",
        "def draw_result_on_frame(frame, result, color, label=None, position='top-left'):\n",
        "    '''\n",
        "    Draw a track/prediction result label and bounding box on an image\n",
        "\n",
        "    Args:\n",
        "        frame: np.array: frame to draw the bounding box on\n",
        "        result: dict: result dict containing class, confidence and bounding box coordinates\n",
        "        color: tuple: BGR color of the bounding box\n",
        "        label: str: label to display (None by default)\n",
        "        position: str: position of the label (top-left, top-right, bottom-left, bottom-right)\n",
        "    \n",
        "    Return:\n",
        "        frame: np.array: modified frame with the bounding box and label drawn on it\n",
        "    '''\n",
        "    \n",
        "    x1, y1, x2, y2 = result['bbox']\n",
        "    \n",
        "    # draw bounding box\n",
        "    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "    # draw label\n",
        "    if label and position in ['top-left', 'top-right', 'bottom-left', 'bottom-right']:\n",
        "        if position == 'top-left':\n",
        "            label_pos = x1, y1-5\n",
        "        elif position == 'top-right':\n",
        "            label_pos = x2, y1\n",
        "        elif position == 'bottom-left':\n",
        "            label_pos = x1, y2\n",
        "        elif position == 'bottom-right':\n",
        "            label_pos = x2, y2-5\n",
        "        \n",
        "        cv2.putText(frame, f\"{label} {result['confidence']}\", label_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "    \n",
        "    return frame\n",
        "\n",
        "def get_bbox_center(bbox):\n",
        "    '''\n",
        "    Get the center coordinates of a bounding box\n",
        "\n",
        "    Args:\n",
        "        bbox: tuple: bounding box coordinates (x1, y1, x2, y2)\n",
        "    \n",
        "    Return:\n",
        "        _: tuple: center of the bounding box (x, y)\n",
        "    '''\n",
        "\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    x = int((x1 + x2) / 2)\n",
        "    y = int((y1 + y2) / 2)\n",
        "    return (x, y)\n",
        "\n",
        "def bbox_inside_bbox(bbox1, bbox2):\n",
        "    '''\n",
        "    Check if a bounding box center is inside another bounding box\n",
        "\n",
        "    Args:\n",
        "        bbox1: tuple: bounding box coordinates (x1, y1, x2, y2)\n",
        "        bbox2: tuple: bounding box coordinates (x1, y1, x2, y2)\n",
        "    \n",
        "    Return:\n",
        "        inside: bool: True if the center of bbox2 is inside bbox1, False otherwise\n",
        "    '''\n",
        "\n",
        "    x1, y1, x2, y2 = bbox1\n",
        "    x_center, y_center = get_bbox_center(bbox2)\n",
        "\n",
        "    inside = x1 < x_center < x2 and y1 < y_center < y2\n",
        "    return inside\n",
        "\n",
        "def combine_bboxes(*args, padding=0):\n",
        "    '''\n",
        "    Combine multiple bounding boxes into a single one framing all of them\n",
        "\n",
        "    Args:\n",
        "        args: tuples: bounding box coordinates (x1, y1, x2, y2)\n",
        "        padding: int: padding to add or substract to the combined bounding box (0 by default)\n",
        "    \n",
        "    Return:\n",
        "        combined_bbox: tuple: combined bounding box coordinates (x1, y1, x2, y2)\n",
        "    '''\n",
        "\n",
        "    combined_bbox = ()\n",
        "    video_w, video_h = 640, 360\n",
        "\n",
        "    for bbox in args:\n",
        "        if not combined_bbox:\n",
        "            combined_bbox = bbox\n",
        "        else:\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            x1_min, y1_min, x2_max, y2_max = combined_bbox\n",
        "            combined_bbox = (\n",
        "                max(min(x1, x1_min) - padding, 0),\n",
        "                max(min(y1, y1_min) - padding, 0),\n",
        "                min(max(x2, x2_max) + padding, video_w),\n",
        "                min(max(y2, y2_max) + padding, video_h)\n",
        "            )\n",
        "\n",
        "    return combined_bbox\n",
        "\n",
        "def track_cyclist_with_helmet(frame, results_list, filter_best=True, debug=False):\n",
        "    '''\n",
        "    Split classification results into 3 groups and detect if a person is riding a bike with a helmet\n",
        "    - Group1: person, man, woman\n",
        "    - Group2: bicycle\n",
        "    - Group3: bicycle helmet, with helmet\n",
        "    \n",
        "    Args:\n",
        "        frame: np.array: image\n",
        "        results_list: list: result dictionaries parsed by the parse_results function\n",
        "        filter_best: bool: True to only display the cyclist with the highest confidence, False to display all cyclists\n",
        "        debug: bool: True to draw all bboxes on the frame, False to only draw the combined one\n",
        "    \n",
        "        Return:\n",
        "            frame: np.array: modified frame with the bounding boxes and labels drawn on it\n",
        "    '''\n",
        "\n",
        "    # Split the results into 3 groups: persons, bycicles, helmets\n",
        "    person_results = [result for result in results_list if result['class'].lower() in  ['person', 'man', 'woman']]\n",
        "    bike_results = [result for result in results_list if result['class'].lower() == 'bicycle']\n",
        "    helmet_results = [result for result in results_list if result['class'].lower() in ['bicycle helmet', 'with helmet']]\n",
        "    \n",
        "    # remove person detections when the top of the bounding touches the frame (helmet disapearing, false negative)\n",
        "    person_results = [person for person in person_results if person['bbox'][1] > 2]\n",
        "\n",
        "    # exit function if no person detected\n",
        "    if not person_results:\n",
        "        return frame\n",
        "\n",
        "    # Order the results by decreasing confidence, to check the most confident predictions first\n",
        "    # in case of overlapping bounding boxes\n",
        "    person_results = sorted(person_results, key=lambda x: x['confidence'], reverse=True)\n",
        "    bike_results = sorted(bike_results, key=lambda x: x['confidence'], reverse=True)\n",
        "    helmet_results = sorted(helmet_results, key=lambda x: x['confidence'], reverse=True)\n",
        "    \n",
        "    # Only take the first person with the max confidence (we can only show one smiley face per frame)\n",
        "    person = person_results[0]\n",
        "\n",
        "    # Check if the person's bounding box contains a bike and a helmet bbox\n",
        "    # We consider that a bounding box is contained inside another if its center is inside the other bbox\n",
        "    bike_found = False\n",
        "    helmet_found = False\n",
        "\n",
        "    if debug:\n",
        "        frame = draw_result_on_frame(frame, person, color=(255, 255, 255), label='Person', position='bottom-right')\n",
        "\n",
        "    # look for a bike bbox inside the person bbox\n",
        "    for bike in bike_results:\n",
        "        if bbox_inside_bbox(person['bbox'], bike['bbox']):\n",
        "            bike_found = True\n",
        "            bike_results.remove(bike)  # remove it to avoid checking it again in case of overlapping bboxes\n",
        "            if debug:\n",
        "                frame = draw_result_on_frame(frame, bike, color=(255, 255, 255), label='Bicycle', position='bottom-right')\n",
        "            break\n",
        "    \n",
        "    # look for a helmet bbox inside the porson bbox\n",
        "    for helmet in helmet_results:\n",
        "        if bbox_inside_bbox(person['bbox'], helmet['bbox']):\n",
        "            helmet_found = True\n",
        "            helmet_results.remove(helmet)\n",
        "            if debug:\n",
        "                frame = draw_result_on_frame(frame, helmet, color=(255, 255, 255), label='Helmet', position='bottom-right')\n",
        "            break\n",
        "    \n",
        "    # biker with helmet\n",
        "    if bike_found and helmet_found:\n",
        "        cyclist = {\n",
        "            'class': 'Cyclist with Helmet', \n",
        "            'confidence': round(mean([person['confidence'], bike['confidence'], helmet['confidence']]), 3),\n",
        "            'bbox': combine_bboxes(person['bbox'], bike['bbox'], helmet['bbox'], padding=3)\n",
        "            }\n",
        "        color = (0, 255, 0)  # green\n",
        "        smiley = cv2.imread(os.path.join(medias_dir, 'smiley_green.png'))\n",
        "\n",
        "\n",
        "    # biker without helmet\n",
        "    elif bike_found and not helmet_found:\n",
        "        color = (0, 0, 255)  # red\n",
        "        cyclist = {\n",
        "            'class': 'Cyclist without Helmet', \n",
        "            'confidence': round(mean([person['confidence'], bike['confidence']]), 3),\n",
        "            'bbox': combine_bboxes(person['bbox'], bike['bbox'], padding=3)\n",
        "            }\n",
        "        smiley = cv2.imread(os.path.join(medias_dir, 'smiley_red.png'), cv2.IMREAD_UNCHANGED)\n",
        "    \n",
        "    # We found a pedestrian or another person object\n",
        "    else:\n",
        "        return frame\n",
        "    \n",
        "    # draw the cyclist bbox and label, greed if cyclist with helmet, red if cyclist without helmet\n",
        "    frame = draw_result_on_frame(frame, cyclist, color=color, label=cyclist['class'], position='top-left')\n",
        "\n",
        "    # draw a smiley face in the bottom right corner\n",
        "    smiley_size = 100\n",
        "    smiley = cv2.resize(smiley, (smiley_size, smiley_size))\n",
        "    frame[-smiley_size:, -smiley_size:] = smiley\n",
        "        \n",
        "    return frame\n",
        "\n",
        "def tracking_thread(frame, model, result_dict, classes, conf):\n",
        "    '''\n",
        "    Track objects on a frame using a given model and store the results in a given dictionary.\n",
        "    We modify the results dictionary in place, as we cannot return it and get the results from the thread.\n",
        "\n",
        "    Args:\n",
        "        frame: np.array: video frame\n",
        "        model: YOLO: YOLO model used to track the objects\n",
        "        result_dict: dict: dictionary passed as reference to store the tracking results\n",
        "        classes: list: list of classes to track\n",
        "        conf: float: confidence threshold\n",
        "    '''\n",
        "    \n",
        "    result_dict[model] = model.track(frame, verbose=False, persist=True, classes=classes, conf=conf)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predict on a Video File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def track_on_video(video_path, tracking_assignment, conf, output_path=None):\n",
        "    '''\n",
        "    Track objects on a video using one or multiple YOLO models and classes.\n",
        "    Each tracking assignment is a tuple containing a YOLO model and a list of classes to track.\n",
        "    The tracking is done on separate threads to speed up the process.\n",
        "\n",
        "    Args:\n",
        "        video_path: str: path to the video file\n",
        "        tracking_assignment: list: list of tuples (model, [classes]) containing the YOLO model and\n",
        "            the classes to track\n",
        "        conf: float: confidence threshold\n",
        "        output_path: str: path to save the tracked video (Skip saving if None)\n",
        "    '''\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    video_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    video_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    debug = False  # debug flag to draw all bboxes\n",
        "\n",
        "    # FPS counter reinitialized every second\n",
        "    fps_count = 0\n",
        "    fps_value = 0\n",
        "    fps_time_counter = datetime.now()\n",
        "\n",
        "    if output_path:\n",
        "        out = cv2.VideoWriter(output_path, -1, video_fps, (video_w, video_h))\n",
        "    \n",
        "    while cap.isOpened(): \n",
        "\n",
        "        # load current frame\n",
        "        success, frame = cap.read()\n",
        "\n",
        "        # restart video if no more frame\n",
        "        if frame is None:\n",
        "            break\n",
        "\n",
        "        # Calculate FPS\n",
        "        fps_count += 1\n",
        "        if datetime.now() - fps_time_counter > timedelta(seconds=1):\n",
        "            fps_value = fps_count\n",
        "            fps_count = 0\n",
        "            fps_time_counter = datetime.now()\n",
        "\n",
        "        # Keyboard shortcuts\n",
        "        key = cv2.waitKey(1) & 0xFF\n",
        "        # toggle debug mode\n",
        "        if key == ord('d'):\n",
        "            debug = not debug\n",
        "        # rewind video\n",
        "        elif key == ord('r'):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "            continue\n",
        "        # quit video\n",
        "        elif key == ord('q'):\n",
        "            break\n",
        "\n",
        "        if success:\n",
        "\n",
        "            # track each model on the frame on a separate thread\n",
        "            results = {}\n",
        "            track_threads = []\n",
        "\n",
        "            # create and start threads\n",
        "            for model, classes in tracking_assignment:\n",
        "                thread = threading.Thread(target=tracking_thread, args=(frame, model, results, classes, conf))\n",
        "                thread.start()\n",
        "                track_threads.append(thread)\n",
        "\n",
        "            # Wait for the threads to finish\n",
        "            for thread in track_threads:\n",
        "                thread.join()\n",
        "\n",
        "            # combine results from all models\n",
        "            result_list = []\n",
        "            for model, result in results.items():\n",
        "                result_list += parse_results(result, model)\n",
        "\n",
        "            # send the combined results to the tracking function to display the bounding boxes\n",
        "            frame = track_cyclist_with_helmet(frame, result_list, filter_best=True, debug=debug)\n",
        "            \n",
        "            # display FPS counter on a black background\n",
        "            frame_h = frame.shape[0]\n",
        "            cv2.rectangle(frame, (0, frame_h-20), (70, frame_h), (0, 0, 0), -1)\n",
        "            cv2.putText(frame, f\"FPS: {fps_value}\", (0, frame_h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "\n",
        "            # display the final frame\n",
        "            cv2.imshow('Video Detection', frame)\n",
        "\n",
        "            # write to video file if output path is provided\n",
        "            if output_path:\n",
        "                out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    if output_path:\n",
        "        out.release()\n",
        "    cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main\n",
        "video_file = 'video2.mp4'\n",
        "video_path = os.path.join(test_videos_dir, video_file)\n",
        "\n",
        "# Define the tracking assignment (which model should track which class)\n",
        "tracking_assignment = [\n",
        "    # (oiv7_yolo, [322, 594, 381, 42, 43]),  # person, man, woman and bike\n",
        "    (coco_yolo, [0, 1]),  # person, bike\n",
        "    (my_yolo, [1])  # helmet\n",
        "]\n",
        "\n",
        "# Predict on the frame\n",
        "# output_video = os.path.join(test_videos_dir, f\"tracked_{video_file}\")\n",
        "output_video = None\n",
        "track_on_video(video_path, tracking_assignment, conf=0.1, output_path=output_video)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
