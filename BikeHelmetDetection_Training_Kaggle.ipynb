{"cells":[{"cell_type":"markdown","metadata":{"id":"jmRWuF3hrt8_"},"source":["# Bike Helmet Detection - Training Notebook\n","\n","In this notebook we will fine-tune YOLOv8n on the public dataset [Helmet Detection](https://www.kaggle.com/datasets/andrewmvd/helmet-detection) to detect if a person is wearing a helmet or not.\n","\n","[Kaggle Notebook](https://www.kaggle.com/code/alexandredj/bikehelmetdetection-yolov8n-training) | [Google Colab Notebook](https://colab.research.google.com/drive/1KGJ68orNqPCK3llccBD6_8MmcEXA1As3)\n","\n","![image14_finetuned.png](readme/image14_finetuned.png)"]},{"cell_type":"markdown","metadata":{"id":"rti4rp1ItWke"},"source":["# Prerequisites"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZfDOwPBoQqW","outputId":"8db5376c-8901-45e9-b7a0-e6c591dca8c2","trusted":true},"outputs":[],"source":["!pip install ultralytics"]},{"cell_type":"markdown","metadata":{"id":"jJ3GHRDh21Yo"},"source":["# Module imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dHbL8rtS2zft","trusted":true},"outputs":[],"source":["import os\n","import shutil\n","import zipfile\n","import cv2\n","import matplotlib.pyplot as plt\n","import xml.etree.ElementTree as ET\n","import random\n","import numpy as np\n","from ultralytics import YOLO\n","from PIL import Image"]},{"cell_type":"markdown","metadata":{"id":"1gOkPuH21or0"},"source":["# Project paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p1BVzWho1n3c","trusted":true},"outputs":[],"source":["# Input\n","input_dir = '/kaggle/input/helmet-detection'\n","images_dir = os.path.join(input_dir, \"images\")\n","annotations_dir = os.path.join(input_dir, \"annotations\")\n","\n","# Output\n","working_dir = '/kaggle/working/'\n","labels_dir = os.path.join(working_dir, \"labels\")\n","train_img_dir = os.path.join(working_dir, \"train\", \"images\")\n","train_labels_dir = os.path.join(working_dir, \"train\", \"labels\")\n","val_img_dir = os.path.join(working_dir, \"val\", \"images\")\n","val_labels_dir = os.path.join(working_dir, \"val\", \"labels\")\n","models_dir = os.path.join(working_dir, \"models\")\n","predict_dir = os.path.join(working_dir, \"predict\")"]},{"cell_type":"markdown","metadata":{"id":"a397E8jt0nPE"},"source":["# Parse XML and preview dataset images with bounding box\n","\n","Load a random image from the dataset, parse bounding box information and draw it on image.\n","\n","Green: With Helmet  \n","Red: Without Helmet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Rf9QM3m0pdD","outputId":"abf9a090-f7b0-4df4-9a3a-0d2ff57c2e0f","trusted":true},"outputs":[],"source":["def parse_xml(xml_file):\n","    '''\n","    Parse the XML file and extract image information and bounding boxes\n","\n","    Args:\n","        xml_file: str: path to the XML file\n","\n","    Return:\n","        image_name: str: name of the image file\n","        image_shape: tuple: shape of the image\n","        labels_and_bboxes: list: list of tuples containing labels and bounding boxes\n","    '''\n","\n","    # Parse the XML file\n","    tree = ET.parse(xml_file)\n","    root = tree.getroot()\n","\n","    # Extract image information\n","    image_name = root.find('filename').text\n","    width = int(root.find('size/width').text)\n","    height = int(root.find('size/height').text)\n","    depth = int(root.find('size/depth').text)\n","    image_shape = width, height, depth\n","\n","    labels_and_bboxes = []\n","\n","    # Loop through each object in the XML\n","    for obj in root.findall('object'):\n","        # Extract label and bounding box coordinates for each object\n","        label = obj.find('name').text\n","        xmin = int(obj.find('bndbox/xmin').text)\n","        ymin = int(obj.find('bndbox/ymin').text)\n","        xmax = int(obj.find('bndbox/xmax').text)\n","        ymax = int(obj.find('bndbox/ymax').text)\n","\n","        # Append label and bounding box to the list\n","        labels_and_bboxes.append((label, (xmin, ymin, xmax, ymax)))\n","\n","    return image_name, image_shape, labels_and_bboxes\n","\n","def draw_bounding_boxes(img_file, labels_and_bboxes):\n","    '''\n","    Draw bounding boxes on the image\n","\n","    Args:\n","        img_file: str: path to the image file\n","        labels_and_bboxes: list: list of tuples containing labels and bounding boxes\n","    '''\n","    # Load the image\n","    image = cv2.cvtColor(cv2.imread(img_file), cv2.COLOR_BGR2RGB)\n","\n","    # Draw bounding boxes on the image\n","    for label, bbox in labels_and_bboxes:\n","        xmin, ymin, xmax, ymax = bbox\n","        rgb_color = (0, 255, 0) if label == 'With Helmet' else (255, 0, 0)\n","\n","        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), rgb_color, 2)\n","        cv2.putText(image, label, (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.35, rgb_color, 1)\n","\n","    # Display the image with bounding boxes\n","    plt.axis(False)\n","    plt.title(os.path.split(img_file)[-1], y=-0.1)\n","    plt.imshow(image)\n","\n","def get_random_img_xml():\n","    '''\n","    Get a random image and its corresponding XML file\n","\n","    Return:\n","        img_file: str: path to the image file\n","        xml_file: str: path to the XML file\n","    '''\n","\n","    # pick a random image from the dataset\n","    img_name = random.choice(os.listdir(images_dir))\n","    img_file = os.path.join(images_dir, img_name)\n","    xml_file = os.path.join(annotations_dir, img_name[:-4]+'.xml')\n","\n","    return img_file, xml_file\n","\n","\n","# Main\n","img_file, xml_file = get_random_img_xml()\n","image_name, image_shape, labels_and_bboxes = parse_xml(xml_file)\n","draw_bounding_boxes(img_file, labels_and_bboxes)"]},{"cell_type":"markdown","metadata":{"id":"x5pNpOduyKQY"},"source":["# Convert input labels\n","The dataset provides PNG images and XML labels using the Pascal VOC format.\n","\n","```xml\n","# Example:\n","<annotation>\n","    <folder>images</folder>\n","    <filename>BikesHelmets1.png</filename>\n","    <size>\n","        <width>400</width>\n","        <height>300</height>\n","        <depth>3</depth>\n","    </size>\n","    <segmented>0</segmented>\n","    <object>\n","        <name>With Helmet</name>\n","        <pose>Unspecified</pose>\n","        <truncated>0</truncated>\n","        <occluded>0</occluded>\n","        <difficult>0</difficult>\n","        <bndbox>\n","            <xmin>161</xmin>\n","            <ymin>0</ymin>\n","            <xmax>252</xmax>\n","            <ymax>82</ymax>\n","        </bndbox>\n","    </object>\n","</annotation>\n","```\n","\n","However, Yolo requires a different label format as input. For each image, a txt file should list the class and the bounding boxes top left and bottom right points, normalized btw 0 and 1.\n","\n","{class} {bbox x1} {bbox y1} {bbox x2} {bbox y2}\n","\n","```\n","# Example:\n","1 0.51625 0.13666666666666666 0.2275 0.2733333333333333\n","```\n","\n","Let's generate our text labels from the xml files."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSuMqZANz8k0","outputId":"9cace24b-20d0-4548-ac3c-efbb486de02e","trusted":true},"outputs":[],"source":["def create_labels(xml_dir, labels_dir):\n","    '''\n","    Create labels for each image in the dataset. Ignore image if invalid bboxes (coordinates out of image shape)\n","\n","    Args:\n","        xml_dir: str: path to the directory containing the annotation xml files\n","        labels_dir: str: path to the directory where the labels will be saved\n","    '''\n","\n","    # browse through annotation xml files and extract the class and bounding box coordinates\n","    os.makedirs(labels_dir, exist_ok=True)\n","\n","    annotations = [file for file in os.listdir(xml_dir) if file.lower().endswith('.xml')]\n","\n","    count = 0\n","    ignored = 0\n","\n","    for xml_file in annotations:\n","\n","        image_name, image_shape, labels_and_bboxes = parse_xml(os.path.join(xml_dir, xml_file))\n","\n","        # save label and bbox to a text file with same name than image file\n","        txt_file = os.path.join(labels_dir, xml_file.replace('.xml', '.txt'))\n","\n","        file_corrupt = False\n","\n","        with open(txt_file, 'w') as f:\n","\n","            for label, bbox in labels_and_bboxes:\n","\n","                # get label\n","                label = 1 if label == 'With Helmet' else 0\n","\n","                # compute bounding box center, width and height from bbox coordinates\n","                x_center = (bbox[0] + bbox[2]) / 2\n","                y_center = (bbox[1] + bbox[3]) / 2\n","                width = bbox[2] - bbox[0]\n","                height = bbox[3] - bbox[1]\n","\n","                # normalize all values between 0 and 1\n","                x_center /= image_shape[0]\n","                y_center /= image_shape[1]\n","                width /= image_shape[0]\n","                height /= image_shape[1]\n","\n","                # check if values are within the range 0 and 1\n","                if x_center > 1 or y_center > 1 or width > 1 or height > 1:\n","                    file_corrupt = True\n","                    break\n","\n","                f.write(f\"{label} {x_center} {y_center} {width} {height}\\n\")\n","\n","        # delete corrupted files (values don't make any sense)\n","        if file_corrupt:\n","            ignored += 1\n","            f.close()\n","            os.remove(txt_file)\n","            continue\n","\n","        print(f\"\\rImage: {image_name}     \", end='', flush=True)\n","        count += 1\n","\n","\n","    print(f\"\\n>> {count} labels created | {ignored} images ignored\")\n","\n","# Main\n","create_labels(annotations_dir, labels_dir)"]},{"cell_type":"markdown","metadata":{"id":"QNc-Wk445t7F"},"source":["# Create Train/Validation sets\n","Split images and labels into a training and validation sets with an 80/20 ratio."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ch-hwznO54ZK","outputId":"c9327ad0-0c40-4146-eb79-876c72b7f442","trusted":true},"outputs":[],"source":["def create_train_val_split():\n","    '''\n","    Create a train and val split of the images/labels.\n","    '''\n","\n","    # make sure target dirs exist\n","    for dir in [train_img_dir, val_img_dir, train_labels_dir, val_labels_dir]:\n","        os.makedirs(dir, exist_ok=True)\n","\n","    # copy images randomly to train and val folders using 80/20 split\n","    images = [img[:-4] for img in os.listdir(images_dir)]\n","    random.shuffle(images)\n","    split = int(0.8 * len(images))\n","\n","    count_total = len(images)\n","    count_train = 0\n","    count_val = 0\n","    count_ignored = 0\n","\n","    for i in range(len(images)):\n","\n","        # check if label exists (some images are corrupted and don't have a label file)\n","        if not os.path.exists(os.path.join(labels_dir, f\"{images[i]}.txt\")):\n","            count_ignored += 1\n","            count_total -= 1\n","            continue\n","\n","        if i < split:\n","            shutil.copy(os.path.join(images_dir, f\"{images[i]}.png\"), train_img_dir)\n","            shutil.copy(os.path.join(labels_dir, f\"{images[i]}.txt\"), train_labels_dir)\n","            count_train += 1\n","        else:\n","            shutil.copy(os.path.join(images_dir, f\"{images[i]}.png\"), val_img_dir)\n","            shutil.copy(os.path.join(labels_dir, f\"{images[i]}.txt\"), val_labels_dir)\n","            count_val += 1\n","\n","        count_total -= 1\n","\n","        print(f\"\\rImages: {count_total} >> Train: {count_train} | Val: {count_val} | Ignored: {count_ignored}     \", end='', flush=True)\n","\n","# Main\n","create_train_val_split()"]},{"cell_type":"markdown","metadata":{"id":"w6JMdI6VoqIV"},"source":["# Generate the training config file\n","\n","To initiate training, yolo requires a yaml file containing the paths to our train and validation sets, as well as our output classes. Let's generate it first."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apbN1XKVCqs-","outputId":"7bad747b-974c-4267-a01d-c9aba9a07684","trusted":true},"outputs":[],"source":["config_file_path = os.path.join(working_dir, 'config.yaml')\n","\n","config_file_contents = f'''path: {working_dir}  # root dir\n","train: train/images  # train dir\n","val: val/images  # val dir\n","\n","# Classes\n","names:\n","  0: without helmet\n","  1: with helmet\n","'''\n","\n","with open(config_file_path, 'w') as f:\n","    f.write(config_file_contents)\n","\n","print(f\"Config file written successfully at location {config_file_path}\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train the model\n","\n","You need to chose a pre-trained model to start from, you will get better results than training it from scratch.  \n","Pick a version from [Ultralytics GitHub](https://github.com/ultralytics/ultralytics).  \n","You can simply enter its name below, it will be downloaded automatically before training.  \n","Below we are fine-tuning YOLOv8n pre-trained on the COCO dataset.  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vy7iRsJ1opBQ","outputId":"f2efa571-7336-46f4-af37-50b858a82cf0","trusted":true},"outputs":[],"source":["# deactivate WandDB if you don't wanna use it\n","os.environ['WANDB_DISABLED'] = 'true'\n","\n","# Load a Yolo8 model, we will use the nano version\n","if not os.path.isfile(config_file_path):\n","    raise FileNotFoundError(f'Config yaml file not found at location {config_file_path}.')\n","\n","# Base model to fine-tune\n","yolo = YOLO('yolov8n.pt')  # default, pre-trained on the Coco dataset\n","\n","yolo.train(\n","    data=config_file_path,\n","    epochs=1,  # 1 as a test, aim for at least 100 epochs\n","    patience=20,  # EarlyStopping\n","    batch=-1,  # automatic batch size\n","    save_period=10,  # save model every 10 epochs\n","    dropout=0.2,  # add some dropout to reduce overfitting\n","    plots=True  # plot results\n",")"]},{"cell_type":"markdown","metadata":{"id":"-pIWQlS9HIsV"},"source":["# Validate model results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2qEKHR3Ac5e","trusted":true},"outputs":[],"source":["valid_results = yolo.val()\n","print(valid_results)"]},{"cell_type":"markdown","metadata":{},"source":["# Testing the model on a picture"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# load latest model\n","model = YOLO(os.path.join(models_dir, 'best_260424_0028.pt'))\n","\n","# load random image\n","img_file, _ = get_random_img_xml()\n","img = Image.open(img_file)\n","\n","# predict on image and save result\n","result = yolo(source=img)[0]\n","os.makedirs(predict_dir, exist_ok=True)\n","img_pred_path = os.path.join(predict_dir, os.path.split(img_file)[-1])\n","result.save(img_pred_path)\n","\n","# load and display the image with the predicted bounding boxes\n","img_pred = cv2.cvtColor(cv2.imread(img_pred_path), cv2.COLOR_BGR2RGB)\n","plt.axis(False)\n","plt.imshow(img_pred)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Testing the model on webcam\n","Grab your helmet and plug-in your webcam, let's try our prediction model on ourselves!  \n","This section won't work on kaggle, download your model and test it locally."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# load latest model\n","model = YOLO(os.path.join(models_dir, 'best_260424_0028.pt'))\n","\n","# Set webcam as source and set resolution\n","cap = cv2.VideoCapture(0)\n","cap.set(3, 640)\n","cap.set(4, 480)\n","\n","# set webcam fps\n","cap.set(cv2.CAP_PROP_FPS, 15)\n","\n","# Load my model\n","model = YOLO(f'{models_dir}/best_260424_0028.pt')\n","\n","while cap.isOpened():\n","    \n","    # load and flip current frame\n","    success, frame = cap.read()\n","    frame = cv2.flip(frame, 1)\n","\n","    if success:\n","        # track on frame and plot result\n","        results = model.track(frame, verbose=False, conf=0.1)\n","        frame = results[0].plot()\n","        cv2.imshow(\"Webcam Tracking\", frame)\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","metadata":{},"source":["# That's it!\n","You can now download your fine-tuned YOLOv8 model and use it to predict on images/webcam/videos."]}],"metadata":{"colab":{"gpuType":"V100","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":687187,"sourceId":1204986,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
